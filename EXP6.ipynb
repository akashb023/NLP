{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBwL2xLNHWcVXfCyWqZ3eK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashb023/NLP/blob/main/EXP6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbj4EUedRkq",
        "outputId": "5163e0f4-c93a-48b5-bed3-4a23510e3d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['Natural Language Processing is interesting.', 'Akash studies Computer Science.']\n",
            "\n",
            "Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'is', 'interesting', '.', 'Akash', 'studies', 'Computer', 'Science', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is interesting. Akash studies Computer Science.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"running better studies\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Lemmatized Words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apSc2jdCd3Tt",
        "outputId": "717b28ff-19f6-414b-919c-b2d7a16290de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words:\n",
            "['running', 'better', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"Akash is studying Computer Science\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(\"POS Tagging:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt8yx0CEd-49",
        "outputId": "4b85725a-8992-4fd1-8431-bcfa2b947225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging:\n",
            "[('Akash', 'NNP'), ('is', 'VBZ'), ('studying', 'VBG'), ('Computer', 'NNP'), ('Science', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "text = \"Akash lives in India and studies at Alliance University\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "ner = ne_chunk(pos_tags)\n",
        "\n",
        "print(\"Named Entity Recognition:\")\n",
        "print(ner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXCxVV5DeHNx",
        "outputId": "11194218-824e-4c71-e3fe-d8a3cecfd3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition:\n",
            "(S\n",
            "  (GPE Akash/NNP)\n",
            "  lives/VBZ\n",
            "  in/IN\n",
            "  (GPE India/NNP)\n",
            "  and/CC\n",
            "  studies/NNS\n",
            "  at/IN\n",
            "  (ORGANIZATION Alliance/NNP University/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is very useful for students\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"After Stop Word Removal:\")\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp9AhLnIeXEl",
        "outputId": "205f7e7f-7220-48f4-caf5-4b7de404b8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stop Word Removal:\n",
            "['Natural', 'Language', 'Processing', 'useful', 'students']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is interesting. Akash studies Computer Science.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lijZC-p_eazM",
        "outputId": "33097d37-3b4f-4bd4-f3c6-71173cc5ec0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['Natural Language Processing is interesting.', 'Akash studies Computer Science.']\n",
            "\n",
            "Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'is', 'interesting', '.', 'Akash', 'studies', 'Computer', 'Science', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"He is the most intelligent guy in the class. He scored ninety percentage.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "words =word_tokenize(text)\n",
        "print(\"word tokenization\")\n",
        "print(words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUbDyXZej-G",
        "outputId": "07eec798-9ee4-4dd8-ec53-5fb464f1147f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['He is the most intelligent guy in the class.', 'He scored ninety percentage.']\n",
            "word tokenization\n",
            "['He', 'is', 'the', 'most', 'intelligent', 'guy', 'in', 'the', 'class', '.', 'He', 'scored', 'ninety', 'percentage', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Experiment 6: Text Processing\n",
        "# ================================\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Step 2: Input text\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
        "Akash is studying Computer Science in India.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "\n",
        "# -------------------------------\n",
        "# Tokenization\n",
        "# -------------------------------\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "print(sentences)\n",
        "\n",
        "print(\"\\nWords:\")\n",
        "print(words)\n",
        "\n",
        "# -------------------------------\n",
        "# Stop Word Removal\n",
        "# -------------------------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"\\nAfter Stop Word Removal:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# -------------------------------\n",
        "# Stemming\n",
        "# -------------------------------\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "print(\"\\nStemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# -------------------------------\n",
        "# Lemmatization\n",
        "# -------------------------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmatized_words)\n",
        "\n",
        "# -------------------------------\n",
        "# Part of Speech (POS) Tagging\n",
        "# -------------------------------\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# -------------------------------\n",
        "# Named Entity Recognition (NER)\n",
        "# -------------------------------\n",
        "ner_tree = ne_chunk(pos_tags)\n",
        "\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(ner_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9s7dAlAjRNe",
        "outputId": "e4ef1250-6921-4864-82fc-62802aff9fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Natural Language Processing (NLP) is a field of Artificial Intelligence.\n",
            "Akash is studying Computer Science in India.\n",
            "\n",
            "\n",
            "Sentences:\n",
            "['\\nNatural Language Processing (NLP) is a field of Artificial Intelligence.', 'Akash is studying Computer Science in India.']\n",
            "\n",
            "Words:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', '.', 'Akash', 'is', 'studying', 'Computer', 'Science', 'in', 'India', '.']\n",
            "\n",
            "After Stop Word Removal:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'field', 'Artificial', 'Intelligence', '.', 'Akash', 'studying', 'Computer', 'Science', 'India', '.']\n",
            "\n",
            "Stemmed Words:\n",
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'field', 'artifici', 'intellig', '.', 'akash', 'studi', 'comput', 'scienc', 'india', '.']\n",
            "\n",
            "Lemmatized Words:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'field', 'Artificial', 'Intelligence', '.', 'Akash', 'studying', 'Computer', 'Science', 'India', '.']\n",
            "\n",
            "POS Tags:\n",
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('.', '.'), ('Akash', 'NNP'), ('is', 'VBZ'), ('studying', 'VBG'), ('Computer', 'NNP'), ('Science', 'NNP'), ('in', 'IN'), ('India', 'NNP'), ('.', '.')]\n",
            "\n",
            "Named Entity Recognition:\n",
            "(S\n",
            "  Natural/JJ\n",
            "  Language/NNP\n",
            "  Processing/NNP\n",
            "  (/(\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  )/)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION Artificial/JJ Intelligence/NNP)\n",
            "  ./.\n",
            "  (PERSON Akash/NNP)\n",
            "  is/VBZ\n",
            "  studying/VBG\n",
            "  (ORGANIZATION Computer/NNP Science/NNP)\n",
            "  in/IN\n",
            "  (GPE India/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K11kVnvqlEYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}